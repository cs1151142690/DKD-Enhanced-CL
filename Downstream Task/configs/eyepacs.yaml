base:
  data_path: '../messidor'
  data_index: null # alternative way to build dataset. check README for more details
  save_path: './result_Messidor_CL'
  log_path: './result_Messidor_CL'
  device: cuda
  random_seed: 1 # set to -1 to disable random seed
  cudnn_deterministic: False # set to True to turn on CUDNN deterministic setting, but it may slow down your training
  overwrite: True # overwrite save_path
  progress: False # real-time metric display, output cannot be redirected

dist:
  distributed: false # distributed data parallel (DDP) model for multiple gpus training
  backend: nccl # default backend for DDP
  nodes: 1 # number of nodes for distributed training 
  n_gpus: null # use all visiable gpus when n_gpus is set to null
  addr: '127.0.0.1' # address used to set up distributed training
  port: '29500' # port used to set up distributed training
  rank: 0 # node rank for distributed training

data:
  num_classes: 4 # number of classes
  input_size: 768 # image size
  in_channels: 3 # number of image channel
  # mean: [0.41849255561828613, 0.25994351506233215, 0.1747538447380066] #enhence
  # std: [0.28423622250556946, 0.18454328179359436, 0.13569211959838867]

  mean: [0.425753653049469, 0.29737451672554016, 0.21293757855892181] # 'auto' or a list of three numbers for RGB
  std: [0.27670302987098694, 0.20240527391433716, 0.1686241775751114] # 'auto' or a list of three numbers for RGB
  sampling_strategy: instance_balanced # instance_balanced / class_balanced / progressively_balanced. ref: https://arxiv.org/abs/1910.09217
  sampling_weights_decay_rate: 0.9 # if sampling_strategy is progressively_balanced, sampling weight will change from class_balanced to instance_balanced
  data_augmentation: # available operations are list in 'data_augmentation_args' below
    - horizontal_flip
    - vertical_flip
    - color_distortion
    - random_crop
    - rotation
    - translation

train:
  network: resnet50 # available networks are list in networks.yaml
  backend: torchvision # network builder backend (timm or torchvision)
  pretrained: true # load weights from pre-trained model training on ImageNet
  checkpoint: './pretrain_models/MIL50_train_fix.pth' # load weights from other pretrained model
  epochs: 25
  batch_size: 4
  num_workers: 2 # number of cpus used to load data at each step
  criterion: cross_entropy # available criterions are list in 'criterion_args' below
  loss_weight: null # null / balance / dynamic / list with shape num_classes. Weights for loss function. Don't use it with weighted sampling!
  loss_weight_decay_rate: 0 # if loss_weights is dynamic, loss weight will decay from balance to equivalent weights
  warmup_epochs: 0 # set to 0 to disable warmup
  kappa_prior: true # save model with higher kappa or higher accuracy in validation set
  save_interval: 5 # the epoch interval of saving model
  eval_interval: 1 # the epoch interval of evaluating model on val dataset
  sample_view: false # save and visualize a batch of images on Tensorboard
  sample_view_interval: 100 # the steps interval of saving samples on Tensorboard. Note that frequently saving images will slow down the training speed.
  pin_memory: true # enables fast data transfer to CUDA-enabled GPUs

solver:
  optimizer: SGD # SGD / ADAM / ADAMW
  learning_rate: 0.001 # initial learning rate
  lr_scheduler: cosine # available schedulers are list in 'scheduler_args' below. please remember to update scheduler_args when number of epoch changes.
  momentum: 0.9 # only for SGD. set to 0 to disable momentum
  nesterov: true # only for SGD.
  weight_decay: 0.0005 # set to 0 to disable weight decay
  adamw_betas: [0.9, 0.999] # for ADAMW optimizer

criterion_args:
  cross_entropy: {}
  mean_square_error: {}
  mean_absolute_error: {}
  smooth_L1: {}
  kappa_loss: {}
  focal_loss:
    alpha: 5
    reduction: mean

# please refer to documents of torch.optim
scheduler_args:
  exponential:
    gamma: 0.6 # multiplicative factor of learning rate decay
  multiple_steps:
    milestones: [15, 25, 45]
    gamma: 0.1 # multiplicative factor of learning rate decay
  cosine:
    T_max: 25 # maximum number of iterations
    eta_min: 0 # minimum learning rate
  reduce_on_plateau:
    mode: min
    factor: 0.1 # new learning rate = factor * learning rate
    patience: 5 # number of epochs with no improvement after which learning rate will be reduced.
    threshold: 0.0001 # threshold for measuring the new optimum
    eps: 0.00001 # minimal decay applied to learning rate
  clipped_cosine:
    T_max: 25
    min_lr: 0.0001

data_augmentation_args:
  horizontal_flip:
    prob: 0.5
  vertical_flip:
    prob: 0.5
  color_distortion:
    prob: 1.0
    brightness: 0.2
    contrast: 0.2
    saturation: 0
    hue: 0
  random_crop: # randomly crop and resize to input_size
    prob: 1.0
    scale: [0.87, 1.15] # range of size of the origin size cropped
    ratio: [0.7, 1.3] # range of aspect ratio of the origin aspect ratio cropped
  rotation:
    prob: 1.0
    degrees: [-180, 180]
  translation:
    prob: 1
    range: [0.2, 0.2]
  grayscale: # randomly convert image to grayscale
    prob: 0.5
  gaussian_blur: # only available for torch version >= 1.7.1.
    prob: 0.2
    kernel_size: 7
    sigma: 0.5
  value_fill: 0 # NOT a data augmentation operation. pixel fill value for the area outside the image
